from contextlib import asynccontextmanager
from fastapi import FastAPI
from langchain_community.vectorstores import Chroma
from langchain_openai import ChatOpenAI
from config import settings
import asyncio

from rag_service.app.nodes import AsyncEmbedder

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ç®¡ç†åº”ç”¨å…¨ç”Ÿå‘½å‘¨æœŸèµ„æº"""
    # ----------------------------
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    # ----------------------------
    print("â³ åˆå§‹åŒ–èµ„æº...")
    
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“è¿æ¥
    app.state.vector_db = Chroma(
        persist_directory=settings.CHROMADB_DIR,
        collection_name=settings.CHROMADB_COLLECTION,
        embedding_function=AsyncEmbedder(model=settings.EMBEDDING_MODEL)
    )
    
    # åˆå§‹åŒ–LLMï¼ˆå¸¦è‡ªåŠ¨æ¸…ç†ï¼‰
    app.state.llm = ChatOpenAI(
        model=settings.OPENAI_MODEL,
        temperature=0.7,
        max_retries=3
    )
    
    # ----------------------------
    # æœåŠ¡è¿è¡Œä¸­ (yield)
    # ----------------------------
    yield
    
    # ----------------------------
    # å…³é—­æ—¶æ¸…ç†
    # ----------------------------
    print("â³ æ¸…ç†èµ„æº...")
    
    # å…³é—­å‘é‡æ•°æ®åº“è¿æ¥
    if hasattr(app.state, 'vector_db'):
        await asyncio.get_event_loop().run_in_executor(
            None,
            lambda: app.state.vector_db.client.close()
        )
        print("âœ… ChromaDB è¿æ¥å·²å…³é—­")
    
    # æ¸…ç†LLMè¿æ¥æ± 
    if hasattr(app.state, 'llm'):
        await app.state.llm.aclose()
        print("âœ… OpenAI è¿æ¥æ± å·²é‡Šæ”¾")
    
    print("ğŸ‰ èµ„æºæ¸…ç†å®Œæˆ")