from contextlib import asynccontextmanager
from fastapi import FastAPI
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from config import settings
import asyncio
import os
from .models.graph_state import GraphState
from .nodes import create_vector_db_node

from app.nodes import AsyncEmbedder

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ç®¡ç†åº”ç”¨å…¨ç”Ÿå‘½å‘¨æœŸèµ„æº"""
    # ----------------------------
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    # ----------------------------
    print("â³ åˆå§‹åŒ–èµ„æº...")
    
    if not os.path.exists(settings.CHROMADB_DIR):
        print("âš ï¸ æœªæ£€æµ‹åˆ°å‘é‡åº“ï¼Œæ­£åœ¨åˆå§‹åŒ–...")
        db_node = await create_vector_db_node(GraphState())
        result = await db_node(GraphState())
        if result["status"] != "success":
            raise RuntimeError(f"åˆå§‹åŒ–å¤±è´¥: {result['message']}")
        
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“è¿æ¥
    app.state.vector_db = Chroma(
        persist_directory=settings.CHROMADB_DIR,
        collection_name=settings.CHROMADB_COLLECTION,
        embedding_function=AsyncEmbedder(model=settings.EMBEDDING_MODEL, base_url=settings.BASE_URL, api_key=settings.API_KEY),
    )
    
    # åˆå§‹åŒ–LLMï¼ˆå¸¦è‡ªåŠ¨æ¸…ç†ï¼‰
    app.state.llm = ChatOpenAI(model=settings.OPENAI_MODEL, 
                               temperature=0.7, api_key=settings.API_KEY, base_url=settings.BASE_URL)

    app.state.logger = settings.logger
    
    # ----------------------------
    # æœåŠ¡è¿è¡Œä¸­ (yield)
    # ----------------------------
    yield
    
    # ----------------------------
    # å…³é—­æ—¶æ¸…ç†
    # ----------------------------
    print("â³ æ¸…ç†èµ„æº...")
    
    print("ğŸ‰ èµ„æºæ¸…ç†å®Œæˆ")